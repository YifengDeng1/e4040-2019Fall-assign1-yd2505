{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: ** [Cross entropy: E=-∑yilog(pj), Softmax loss: L=-∑yilog(sj), if pj = sj,the output of Softmax loss and cross entropy are the same. if we using the softmax function as the output, pj=sj, it can be used to calculate the loss of softmax classifier. ]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: **[\n",
    "   it depends on the number of class, and the class determine the matrix of W.We just use different function of L to compute.\n",
    "   we can get the method by connecting logistic regression with softmax,becuase the LR is a simple SR\n",
    "   Multi-class and binary logistic regression are simple model to do classification.\n",
    "   ]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **[Because its gradients is none-satuation, it conveges faster in SGD comparing to other actvation function; it does not need expensive computation like exp() and it won't have vanishing or exploding gradients problem]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: **[For a classification or refression problem, suppose there are N avaliable models M={M1,M2,..Mi,...MN},k-fold cross  validation sets 1/k of the training set as test set, train k times and test k times for each model, the error rate is the average, and the best model is the one with lowest average error rate.\n",
    "   Implementation:\n",
    "   1. Devide the training set into k seperate sub-set, say there are totally m training examples, each sub-set has m/k examples, the sub-sets are called S.\n",
    "   2.Each time, take a Mi from M, select all but one Sj from S, use the selected k-1 numbers of Si to train Mi to get hypothesis function Hij, then use Sj to test, and get an experience error.\n",
    "   3. Totally for a Mi, we can get k numbers of experience errors, thus the experience error for Mi is the average of all k experience errors.\n",
    "   4. Chooese the best model: the one with minimum experience error. Lastly train with all S,and get error Hi. ]]**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **[Accuracy: 0.5078 best model: \n",
    "   hidden_dim=160, num_classes=10, reg=1e-4, weight_scale=1e-3,num_epoch = 21,batch_size = 600,learning_rate = 1e-3\n",
    "   My starting point is the same as the default given hyperparameters. First I increased the hidden dimension, the accuracy increased a little bit, than with current best hidden dimension, I increased the number of epoch to around 30, the accuracy increased at first, but later seems overfitting after around 22th epoch, so I set num_epoch=21. Then I increased learning rate, at 1e-3 the accuracy achieved more than 50%.\n",
    "Results: (complete results is shown in task2)\n",
    "number of batches for training: 81]\n",
    "number of batches for training: 81\n",
    "\n",
    "48000/49000 loss: 2.0140428192345627\n",
    "epoch 1: valid acc = 0.258, new learning rate = 0.00095\n",
    "\n",
    "epoch 2: valid acc = 0.37, new learning rate = 0.0009025\n",
    "\n",
    "epoch 3: valid acc = 0.41, new learning rate = 0.000857375\n",
    "\n",
    "epoch 4: valid acc = 0.443, new learning rate = 0.0008145062499999999\n",
    "\n",
    "epoch 5: valid acc = 0.452, new learning rate = 0.0007737809374999998\n",
    "\n",
    "epoch 6: valid acc = 0.468, new learning rate = 0.0007350918906249997\n",
    "\n",
    "epoch 7: valid acc = 0.479, new learning rate = 0.0006983372960937497\n",
    "\n",
    "epoch 8: valid acc = 0.48, new learning rate = 0.0006634204312890621\n",
    "\n",
    "epoch 9: valid acc = 0.487, new learning rate = 0.000630249409724609\n",
    "\n",
    "epoch 10: valid acc = 0.48, new learning rate = 0.0005987369392383785\n",
    "\n",
    "epoch 11: valid acc = 0.491, new learning rate = 0.0005688000922764595\n",
    "\n",
    "epoch 12: valid acc = 0.496, new learning rate = 0.0005403600876626365\n",
    "\n",
    "epoch 13: valid acc = 0.516, new learning rate = 0.0005133420832795047\n",
    "\n",
    "epoch 14: valid acc = 0.512, new learning rate = 0.00048767497911552944\n",
    "\n",
    "epoch 15: valid acc = 0.511, new learning rate = 0.00046329123015975297\n",
    "\n",
    "epoch 16: valid acc = 0.507, new learning rate = 0.0004401266686517653\n",
    "\n",
    "epoch 17: valid acc = 0.522, new learning rate = 0.00041812033521917703\n",
    "\n",
    "epoch 18: valid acc = 0.515, new learning rate = 0.00039721431845821814\n",
    "\n",
    "epoch 19: valid acc = 0.514, new learning rate = 0.0003773536025353072\n",
    "\n",
    "epoch 20: valid acc = 0.517, new learning rate = 0.0003584859224085418\n",
    "\n",
    "epoch 21: valid acc = 0.506, new learning rate = 0.0003405616262881147\n",
    "test acc: 0.5078\n",
    "**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[ your answer]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
